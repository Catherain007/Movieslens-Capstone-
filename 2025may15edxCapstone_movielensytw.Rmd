---
title: "EDX Data Science Capstone - MovieLens Project"
author: "Yin Thu Win"
date: "5/15/2025"
output:
  word_document: default
 
---

```{r, Global Settings, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.width= 6, fig.height=4) 
```

# 1.Introduction:

| 

The MovieLens project is a data science initiative that involves analyzing a large dataset of movie ratings, with the aim of building machine learning models to predict user preferences and enhance movie recommendation systems. The dataset, developed by GroupLens Research at the University of Minnesota, contains millions of ratings from users on a wide range of films.

In this project, we will explore the MovieLens data, perform exploratory data analysis (EDA), and develop machine learning models to predict movie ratings based on various factors, including user preferences, movie characteristics, and the age of the films. We will evaluate model performance using metrics such as Root Mean Squared Error (RMSE) and aim to improve the accuracy of predictions by incorporating regularization and various user and movie effects. Ultimately, the goal is to build a recommendation system that provides accurate and personalized movie suggestions.

Since the outbreak of the COVID-19 pandemic, user behaviour of internet and entertainment industry has transformed significantly for health, work, social interaction, education, leisure, shopping, and information sharing. As the e-commerce industry trends has rapidly grow exponentially, it is clear that recommendation systems have become a crucial component of online applications, helping to manage data velocity, variability, and volume effectively.

These recommender engines are commonly used in platforms like Netflix, Amazon, and YouTube, where they provide personalized recommendations tailored to each user.In essence, machine learning powers recommender engines by analyzing user data, identifying patterns, and continually improving the quality and relevance of suggestions, benefiting both users and businesses alike. In summary, while recommendation systems offer significant advantages, addressing these challenges of handle big data, changing user behaviour, Overfitting, that requires continuous innovation and improvement to ensure they remain effective, accurate, and fair.

[My Github Repo](https://github.com/Catherain007/Movieslens-Capstone-)

# 2. Methods and Analysis:

|        We will start by preparing the data and loading it from the [GroupLens Website](https://grouplens.org/datasets/movielens/10m/). The data will be divided into two sets, "edx" and "validation," before being further split into training and test sets. A preliminary exploratory analysis will be performed to review the dataset's features and identify any potential biases that could impact and reduce the predictive accuracy of our models.


The dataset will be cleaned to remove any NAs and ensure it is in a tidy format. Data visualization, including histograms and scatter plots, will be incorporated into the exploratory analysis and modeling to enhance the presentation.

The modeling approach will be guided by insights from the exploratory analysis and refined to develop a Final Validation Model with an RMSE below 0.8649. This final model will account for User, Movie, and Movie-Age Effects, incorporating regularization to achieve the desired performance.

First, the edX training and validation datasets are created by downloading data from the MovieLens 10M dataset. Exploratory data analysis (EDA) is then conducted on various independent variables (predictors). The individual effects or biases of these predictors related to the ratings are introduced and analyzed using the group_by function on the training edX dataset. A simple linear regression model is developed for each predictor's effect in the proposed models. Additionally, combinations of predictors are explored to identify potential improvements. A correlation matrix is used to identify the most influential predictors for the predicted outcome. Regularization is applied to determine the optimal tuning parameters on the edX dataset, with appropriate data partitioning. The performance of all proposed models is evaluated using the final hold-out validation test set to calculate RMSE scores. Finally, the best-performing model with the lowest RMSE score is revealed.

# 2.1 Data Preparation and Required Packages

```{r message=FALSE, warning=FALSE}
#Install the pacman package
if(!require(pacman)) install.packages("pacman", repos = "http://cran.us.r-project.org")
#Load the required libraries
#If a package below is missing, p_load will automatically download it from CRAN
pacman::p_load(tidyverse, ggplot2, ggthemes, data.table, lubridate, caret, 
               knitr, scales, treemapify)
#All Data for the MovieLens Dataset Will be obtained from the following sources:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip
#Data Preparation
#Download File

library(lubridate)
library(data.table)
library(corrplot)
library(corrr)
library(tinytex)



```

```{r, Pull Data from MovieLens Website}

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")



### if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
title = as.character(title),
genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")


### Validation set will be 10% of MovieLens data


set.seed(1, sample.kind ="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]


### Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

### Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

#### remove uncessary file for better processing time
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

# 2.1.1 Exploratory Data Analysis

After deploying codes provided for this project, it is shaped into the edx training dataset consist of 90% of the data and the validation testing dataset consist of 10% of data. It is found that no missing values in any column.

```{r   check NA }

#### Check missing values in any column.
sapply(edx, {function(x) any(is.na(x))}) %>% knitr::kable()


```

Data set dimension as shown below.

```{r   Dimensions }
####To get dimension of data matrix 
dim(edx)
```

Confirm the data is tidy:

```{r}
edx %>% as_tibble()
```

## Explore the features and classes of edx while also confirming its observations

```{r}
glimpse(edx)
```

# Deployment for the unique number of userIds, movieIds, and genres

```{r, fig.align="center"}
edx %>% summarize(unique_users = length(unique(userId)),
                  unique_movies = length(unique(movieId)),
                  unique_genres = length(unique(genres)))
```

#Let's visually analyze the distribution of ratings provided by users.:


# 2.2 Ratings:

Deployment of 10 different ratings creation to recommend the movies:

```{r}
length(unique(edx$rating))
```


```{r}
rp <-edx %>% filter(edx$rating >=3)
nrow(rp)/length(edx$rating)
```

```{r   av and med }
##### Data Visualizations and Analysis
 
avge_rating <- mean(edx$rating) # calculate overall average rating of whole edx data set
medi_rating <- median(edx$rating) # calculate median rating of whole edx data set
```

Overall average rating for edx data set is 3.5126, Whole Median rating for edx data set is 4.

Quantitative features userId: discrete, Unique ID for the user movieId: discrete, Unique ID for the movie timestamp: discrete, Date and time the rating was given Qualitative features title: nominal, movie title (not unique) genres: nominal, genres associated with the movie Outcome, Y rating: continuous, a rating between 0 and 5 with 0.5 increment for the movie


The rating data given by users to the particular movies is analysis by group of rating. Rating included whole star 1 to 5 scale and half star rating from 0.5 to 4.5. Frequency of each rating are summarized with their whole and half star rating groups. According to the data visualization, the user more likely to give whole star rating than half star rating. Users give highest number rating frequency with 4 stars rating and 3, 5, 3.5, 2 stars rating accordingly. The majority of user prefer to give higher rating on the movie they reviewed.

```{r rating}
######RATING

  edx_ratings <- edx %>% # take data from edx and. assign new data set.. 
  group_by(rating) %>% # ...group data by rating and... 
  summarize(num_ratings = n()) %>% # ...summarize frequency of each rating and... 
  arrange(desc(num_ratings)) # ...arrange data in descending order

edx_ratings # display rating frequencies
edx_ratings %>% # take data and..... 
  ggplot(aes(rating, num_ratings)) + # scatter plot rating vs frequency ........and
  geom_point(aes(size = num_ratings)) + #  display rating point size with number of ratings.......and.
    scale_size_continuous(limits = c(0, 7e+06)) + # set the scale in Y........and 
  xlim(0,5) + # set the scale in x....and
  labs(x = "Individual Rating", y = "Total Ratings", title = "Total Ratings by Individual Rating")
            # puting the lables 
```


The distribution of ratings in the dataset reveals interesting trends regarding user preferences. The most popular rating is 4.0, with 2,588,430 occurrences, followed by 3.0 with 2,121,240 ratings. These indicate that users tend to rate films more positively, with a higher concentration of ratings clustered around the middle to upper range.

Ratings of 5.0 are also common, with 1,390,114 occurrences, reflecting the presence of highly rated films. On the other hand, ratings on the lower end of the scale, such as 1.0, 2.0, and 0.5, are awarded significantly less frequently, with 345,679, 711,422, and 85,374 occurrences, respectively. The 0.5 rating, in particular, is the least frequent, suggesting that users rarely give the lowest possible rating.

This distribution indicates a general tendency for users to avoid giving extremely low ratings, possibly reflecting a bias towards more moderate or positive feedback. The presence of a relatively high number of 3.5 and 4.5 ratings further supports this notion, as users appear more inclined to rate films in the upper-middle range rather than at the extremes.


## 2.2.1 User

User is a critical feature in providing ratings for movies at specific times. The edx dataset contains 69,878 unique users. We analyze the user data by grouping it by userId and summarizing the frequency of their ratings in descending order. A visual exploration of the number of ratings per userId and the average rating reveals the following relationships.

```{r d user}
edx %>%  # finding unique userId in data set 
  
  summarize(num_users = n_distinct(userId))
```

```{r userI}
edx_users <- edx %>% # take data from edx and. assign new data set.. 
  group_by(userId) %>% # ...group by user and... 
  summarize(num_ratings = n(), avg_rating = mean(rating)) %>% # ...summarize ratings counts and average rating and... 
  arrange(desc(num_ratings)) # ...arrange data in descending order



edx_users %>%
  ggplot(aes(x = userId, y = num_ratings, color = avg_rating)) +  # Scatter plot with userId and number of ratings, colored by average rating
  geom_point(size = 3, alpha = 0.6) +  # Adjust size and transparency for visibility
  scale_color_gradientn(colours = rainbow(6)) +  # Apply a color gradient to represent average rating
  labs(
    title = "Number of Ratings by UserId",  # Main title
    x = "UserId",  # Label for the x-axis
    y = "Number of Ratings",  # Label for the y-axis
    color = "Average Rating"  # Color legend title
  ) +
  theme_minimal() +  # Apply minimal theme for a clean look
  theme(
    plot.title = element_text(size = 16, face = "bold", color = "darkblue", hjust = 0.5),  # Title styling
    axis.text.x = element_text(size = 10, angle = 45, hjust = 1, color = "darkred"),  # Rotate x-axis labels and style them
    axis.text.y = element_text(size = 12, color = "darkgreen"),  # Style y-axis labels
    panel.grid.major = element_line(color = "gray", size = 0.3),  # Grid lines for the plot
    panel.grid.minor = element_blank()  # Remove minor grid lines for a cleaner appearance
  )
```

Some users are more active than others when it comes to reviewing and rating movies. Specifically, the following provides a detailed view of the rating behavior of the top and bottom 10 users, highlighting their rating frequency.

```{r tuser}

top_10users<-head(edx_users,10) # assign data to new data set

lowest_10users<-tail( edx_users,10) # assign data to new data set

plot(top_10users) # correlation plot for top 10 user

 plot(lowest_10users) # correlation plot for buttom 10 user
```

According visualize data, top 10 user rate around 2500 to 6600 movies and their average rating range from 2.4 to 3.6. However, bottom 10 user rate 10 to 14 movies only and their average rating rage from 2.5 to 4.5. This observation proved that there has variation in rating of the movies by different users.

## 2.2.2 Movie

Movies are a key feature in determining ratings from different users for a particular movieId. Each year, new movies are released, and ratings for these movies are accumulated over the following years, depending on the users. The given edx dataset contains 10,677 movies. To further analyze the movie data, we group the movies by their movieId and summarize the frequency of their ratings in descending order.

```{r movieI}
edx %>% # finding unique movieId in data set 
  summarize( n_movies = n_distinct(movieId))

edx_films <- edx %>% # take data and assign new data set and... 
  group_by(movieId) %>% # ...group by movie and... 
  summarize(num_ratings = n(), avg_rating = mean(rating)) %>% # ...summarize ratings counts and average rating and... 
  arrange(desc(num_ratings)) # ...arrange data in descending order


edx_films %>% 
  ggplot(aes(x = movieId, y = num_ratings, color = avg_rating)) +  # Plot movieId vs number of ratings with color by average rating
  geom_point(size = 3, alpha = 0.6) +  # Add points with adjusted size and transparency for better visualization
  scale_color_gradientn(colours = terrain.colors(55)) +  # Apply a 5-color terrain gradient for better color differentiation
  labs(
    x = "Movie ID",  # Label for the x-axis
    y = "Number of Ratings",  # Label for the y-axis
    title = "Ratings by Movie ID",  # Title of the plot
    color = "Average Rating"  # Label for the color legend
  ) +
  theme_light() +  # Apply a light theme for a clean and modern look
  theme(
    plot.title = element_text(size = 18, face = "bold", color = "darkblue", hjust = 0.5),  # Title styling with bold and centered alignment
    axis.text.x = element_text(size = 10, angle = 45, hjust = 1, color = "darkred"),  # Rotate x-axis labels and adjust their style
    axis.text.y = element_text(size = 12, color = "darkgreen"),  # Style y-axis labels
    panel.grid.major = element_line(color = "gray", size = 0.5),  # Major grid lines with gray color
    panel.grid.minor = element_blank()  # Remove minor grid lines for a cleaner look
  )
```

The relationship between movieId and number of ratings with average rating show that some popular movies get huge rating than other ordinary or infamous movie. Additionally, there are some movieId not get rating or not in the data set can be found. Hence, we make another plot of number rating with vs unique movie and it can see clearer picture of different rating between movies.

```{r movieu}

####plot with unique movie 
edx_films %>%
  mutate(row_number = 1:n()) %>%  # Create a unique identifier for each movie using row numbers
  ggplot(aes(x = row_number, y = num_ratings, color = avg_rating)) +  # Scatter plot: row number (movie ID) vs. number of ratings
  geom_point(size = 3, alpha = 0.7) +  # Points with size adjustment and slight transparency
  scale_color_gradientn(colours = terrain.colors(7)) +  # Apply a color gradient based on average ratings
  labs(
    x = "Movie ID",  # X-axis label
    y = "Total Ratings",  # Y-axis label
    title = "Distribution of Ratings Across Movies",  # Main plot title
    color = "Average Movie Rating"  # Color legend label
  ) +
  theme_light() +  # Light theme for a clean, modern look
  theme(
    plot.title = element_text(size = 18, face = "bold", color = "navy", hjust = 0.5),  # Bold title with centered alignment
    axis.text.x = element_text(size = 10, angle = 45, hjust = 1, color = "darkorange"),  # Rotated X-axis labels with adjusted size
    axis.text.y = element_text(size = 12, color = "darkgreen"),  # Styled Y-axis labels with custom size and color
    panel.grid.major = element_line(color = "gray80", size = 0.3),  # Lighter gray for major grid lines
    panel.grid.minor = element_blank(),  # Remove minor grid lines for a cleaner appearance
    plot.margin = margin(20, 20, 20, 20)  # Increase plot margins for better readability
  )
                       # putting the labels
```

Top 10 number of rated movies can be discovered in the following.

```{r top 10 movie}
# the data frame top_title contains the top 10 movies which count the major number of ratings
top_title <- edx %>% # take the data .....and
  group_by(title) %>% # group by movie title .....and
  summarize(count=n()) %>% # ...summarize ratings counts ...and 
  top_n(10,count) %>% # take top 10 rating numbers..and
  arrange(desc(count)) # arrange data in decent order

#Horizontal bar chart of top_10 title of  movies 



top_title %>%  # Take the data for movie titles and rating counts
  ggplot(aes(x = reorder(title, count), y = count)) +  # Reorder the titles based on the count of ratings
  geom_bar(stat = "identity", fill = "#FF6347", color = "black", width = 0.7) +  # Create a bar plot with a tomato color and black borders
  coord_flip(ylim = c(0, 40000)) +  # Flip the coordinates and set the y-axis range
  geom_text(aes(label = count), hjust = -0.1, size = 3.5, color = "black") +  # Add rating counts with better spacing and color
  labs(
    title = "Top 10 Movies Based on Number of Ratings",  # Add a modernized title
    x = NULL,  # Remove the x-axis label for cleaner presentation
    y = "Number of Ratings"  # Add y-axis label
  ) +
  theme_minimal() +  # Use a minimalistic theme
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5, color = "darkblue"),  # Customize the title
    axis.text.x = element_text(angle = 45, hjust = 1, color = "darkgreen"),  # Rotate x-axis labels and customize color
    axis.text.y = element_text(color = "darkred"),  # Customize y-axis label color
    panel.grid.major = element_line(color = "gray", size = 0.5, linetype = "dashed"),  # Add a dashed grid for a modern touch
    panel.grid.minor = element_blank()  # Remove minor grid lines
  )
```

## 2.2.3 Genres

A genre refers to a category of creative work, including literature, art, entertainment, and movies. In the edx dataset, there are 20 unique genres, and some movies belong to multiple genres. After analyzing the data using the group_by function, we identified 797 unique genre combinations. If we were to split every unique genre combination, it would result in millions of additional rows, which could cause the computer to crash during processing. Therefore, for this project, we will only use the default genres provided in the dataset. Below, you will find the top 10 and bottom 10 genres based on the analysis.

```{r GenresI}

###GENRES


edxgenres<-edx%>% # take data from edx and assign new data set
  group_by(genres) %>% # ...group data by genre and... 
  summarize(num_ratings = n(), avg_rating = mean(rating)) %>% # ...summarize ratings counts and average rating and... 
  arrange(desc(num_ratings)) # ...arrange data in descending order

head(edxgenres,10) # show top 10 rating genere

top_10g<-head(edxgenres,10)  # assign top genere data to new data set


tail(edxgenres,10) # show lowest 10 rating genere


top_10g %>%  # Take the data for top 10 genres
  ggplot(aes(x = genres, y = num_ratings, color = avg_rating)) +  # Scatter plot between genres and number of ratings, with color based on average rating
  geom_point(size = 7) +  # Plot the points with a size of 4 for visibility
  scale_colour_gradientn(colours = rainbow(7)) +  # Set the color gradient to a rainbow color palette
  labs(x = "Genre", y = "Number of Ratings", title = "Ratings by Genre") +  # Add labels for x, y, and title
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, color = "darkred", size = 10),  # Rotate x-axis labels to 45 degrees, change color to darkred, and adjust size
    plot.title = element_text(hjust = 0.5, color = "darkblue", size = 14, face = "bold"),  # Center the title and customize its appearance
    plot.subtitle = element_text(hjust = 0.5, color = "green", size = 12),  # Center subtitle and change its color
    panel.background = element_rect(fill = "darkgray"),  # Set the panel background to light gray
    panel.grid.major = element_line(size = 0.5, linetype = "solid", color = "gray"),  # Major grid lines in solid gray
    panel.grid.minor = element_line(size = 0.25, linetype = "dotted", color = "lightgray")  # Minor grid lines in dotted light gray
  )
```

We will focus on the top genres that have received more than 100,000 ratings. For each of these genres, we calculate the average rating and standard error, then plot these values as error bar plots. The resulting plot highlights the variation in ratings across genres, indicating genre-specific differences in the number of ratings.

```{r GenresT}

#An error bar plots for genres with more than 100000 ratings

edx %>% 
  group_by(genres) %>%  # Group the data by genres
  summarize(n = n(), 
            avg_rating = mean(rating), 
            se = sd(rating) / sqrt(n())) %>%  # Calculate the number of ratings, average rating, and standard error
  filter(n >= 100000) %>%  # Filter genres with ratings greater than or equal to 100,000
  mutate(genres = reorder(genres, avg_rating)) %>%  # Reorder genres by average rating
  ggplot(aes(x = genres, y = avg_rating, ymin = avg_rating - 2 * se, ymax = avg_rating + 2 * se)) + 
  geom_point(color = "darkblue") +  # Plot the genres vs. average rating with points
  geom_errorbar(width = 0.7, color = "red") +  # Add error bars to represent the standard error
  theme(axis.text.x = element_text(angle = 45, hjust = 1, color = "darkgreen", size = 10)) +  # Rotate x-axis labels and customize appearance
  labs(title = "Error Bar Plots by Genres", subtitle = "Average Ratings with Standard Error") +  # Set the title and subtitle
  theme(  # Set a custom background and grid style
    panel.background = element_rect(fill = "skyblue", color = "darkred", size = 1),
    panel.grid.major = element_line(size = 0.9, linetype = "dotted", color = "green"),
    panel.grid.minor = element_line(size = 0.9, linetype = "dotted", color = "yellow"),
    plot.title = element_text(hjust = 0.5, color = "darkblue", size = 14, face = "bold"),  # Center and customize title
    plot.subtitle = element_text(hjust = 0.5, color = "purple", size = 12)  # Center and customize subtitle
  )
```

## 2.2.4 Time

In the edx dataset, the timestamp represents the time at which users rated different movies, with the units measured in seconds since January 1, 1970. To enhance our data analysis, we create a new column called "date" using the round_date function and add it to both the edx and validation datasets. We then compute the average rating for each week and visualize this average rating over time using a plot.

```{r time }
##### TIME 

#ggplot showing timestamp per date(week unit)
  edx %>% 
    mutate(date = round_date(as_datetime(timestamp), unit = "week")) %>%  # Add a new 'date' column with rounded dates
    group_by(date) %>%  # Group by the 'date' column
    summarize(avg_rating = mean(rating)) %>%  # Calculate the average rating for each week
    ggplot(aes(x = date, y = avg_rating)) + 
    geom_line(color = "blue") +  # Line plot for the average rating over time
    geom_point(color = "red") +  # Add red points on the line for each data point
    ggtitle("Timestamp, Time Unit: Week") +  # Add a main title
    labs(subtitle = "Average Ratings") +  # Add a subtitle
    theme_minimal()  # Apply a minimal theme for better aesthetics


```

Analysing the trend of the average ratings versus the timestamp, there is significant effect in the early two year of the plot and then after 1996, average ratings were populated widely. After 1998, surfing internet started popular and movie average ratings have much improvement noticeable. It can be seen that, after 2000 year the most of the average rating occurred between 3 and 3.5 respectively.

# 2.3 Developing the Linear Regression Models

In this machine learning project, each outcome Y is associated with a different set of predictors. Specifically, user u provides a rating for movie i at time stamp t, and the popularity of the movie, categorized by genres g, is also considered as a predictor for the outcome Y.

## 2.3.1 Simple linear regression model (Average)

We begin with a simple approach to build our linear regression models, where the same rating is predicted for all movies, regardless of the users or other related independent variables. This represents the most basic form of a recommendation system.

$$
Y_{u,i} = \mu + \varepsilon_{u,i}--(1)
$$

Where, $\varepsilon_{i,u}$ independent errors sampled from the same distribution centred at 0 $\mu$ the "true" rating for all movies (the average of all ratings)

```{r overall mean }
mue <- mean(edx$rating)
mue
```

##  Considering individual effect of predictors to the first model

### 2.3.2 Movie Effect Model

It is well-known that different movies receive varying ratings, with a clear variation between them. In this context, we use the average rating of a movie effect denote as $e_i$ in simple model. We can add augment our first model by adding the term $e_i$ to represent average ranking for movie $i$.

$$
Y_{u,i} = \mu + e_i + \varepsilon_{u,i}              
$$

Where, $\mu$ the “true” rating for all movies $e_i$ is effects or bias, movie-specific effect $\varepsilon_{u,i}$ independent errors sampled from the same distribution centered at 0

```{r movie effect}
####movie effect (e_i) determine bias for each movie (mean rating of the movie compare to overall mean)

movie_avgs <- edx %>% # take data from edx....and assign new data set
  group_by(movieId) %>% # group by movieId..and 
  summarise(e_i = mean(rating) - mue) # summarize result of movie effect...

#### Movie bias/effect plots

EM_hist <- movie_avgs %>% #assign data to new data set....and
  ggplot(aes(e_i)) +  #plot movie bias histrogram vs movie count......
  geom_histogram(color = "blue", fill="gray",bins=40) + #select bin size and fill color
  ggtitle("Movie Effect Distribution Histogram") +
  xlab("Movie Bias") +
  ylab("Movie Count") # put all labels 
  
plot(EM_hist) # plot the histogram data of bias

```

The left screw distribution illustrates the movie effect/bias.

### 2.3.3 User Effect Model

We also observed that some users are more active in providing ratings, while others are less active and rate only a few movies. This results in a clear variation in the level of engagement between users. Here, we use the average ranking of user effect denote as $e_u$. We can add augment our simple average model by adding the term $e_u$ to represent average ranking for user $u$.

$$
Y_{u,i} = \mu + e_u + \varepsilon_{u,i}
$$

where , $e_u$ is effects or bias, user-specific effect $\mu$ and $\varepsilon_{u,i}$ are defined as in (1)

```{r user effect}
#####User effect (e_u) determine bias for each user (mean rating of the user compare to overall mean)

user_avgs <- edx %>% # take data from the edx. and assign as new data set.....
  group_by(userId) %>%  # make group by userID.....and 
  summarise(e_u = mean(rating) - mue) # summarize result of user effect...

#### User bias/effect plots

EU_hist <- user_avgs %>%  #assign user avgerate rating to new data set....and
    ggplot(aes(e_u)) +    # plot user bias histrogram vs user count......
  geom_histogram(color = "blue", fill="gray", bins=40) + #select bin size and fill color
  ggtitle("User Effect Distribution Histogram") +
  xlab("User Bias") +
  ylab("User Count")  # put all labels 
 
plot(EU_hist) # plot the histogram data of bias

```

The right screw distribution presents the user effect/bias.

### 2.3.4 Genres Effect Model

It is obvious that certain movie genres receive significantly higher ratings than others, due to the varying popularity of movies within those genres among reviewers. Here, we use the average ranking of genres effect denote as $e_g$. We can add augment our simple average model by adding the term $e_g$ to represent average ranking for genres $g$.

$$
Y_{u,i} = \mu + e_g + \varepsilon_{u,i}
$$

where: $e_g$ is effects or bias, Genres-specific effect $\mu$ and $\varepsilon_{u,i}$ are defined as in (1)

```{r Genres Effect}

#### Time effect (e_g) determine bias for each genres group (mean rating of the genre compare to overall mean)
gene_avgs <- edx %>% # Take  data from edx....and assign new data set
  group_by(genres) %>% # Group by (genres)
  summarise(e_g = mean(rating) - mue) # summarize result of genres effect...


##### Genre bias/effect plots

EG_hist <- gene_avgs %>%  #assign data to new data set
  ggplot(aes(e_g)) + #plot genre bias histrogram vs genres count......
  geom_histogram(color = "blue", fill="gray",bins=40) + #select bin size and fill color
  ggtitle("Genres Effect Distribution Histogram") +
  xlab("Genres Bias") +
  ylab("Genres Count") # put all labels 

plot(EG_hist) # plot the histogram data of bias
```

The left screw distribution presents the genres effect/bias.

### 2.3.5 Time Effect Model

We also observed significant variation in ratings from certain earlier years, indicating the presence of temporal trends in rating behavior over specific time periods. Here, we use the average ranking of time effect denote as $e_g$. We can add augment our simple average model by adding the term $e_g$ to represent average ranking for time $t$.

$$
Y_{u,i} = \mu + e_t + \varepsilon_{u,i}
$$

Where, $e_t$is effects or bias, Time-specific effect $\mu$ and $\varepsilon_{u,i}$ is defined as in (1)

```{r Time effect}

##### Time effect (e_t) determine bias for each rate time by week (mean rating of the time compare to overall mean)

## put additional column name date in edx data set 

edx <- edx %>% mutate(date = round_date(as_datetime(timestamp), unit = "week")) 

time_avgs <- edx %>% #assign data to new data set
  group_by(date) %>%  # group by date 
  summarise (e_t = mean(rating) - mue ) # summarize result of time effect...

# Time effect/ bias plot 

ET_hist <- time_avgs %>% #assign data to new data set
  ggplot(aes(e_t)) + #plot timebias histrogram vs time count.....
  geom_histogram(color = "blue", fill="gray",bins=40) + #select bin size and fill color
  ggtitle("Time Effect Distribution Histogram") +
  xlab("Time Bias") +
  ylab("Time Count") + ggtitle("Timestamp, time unit : week") # put all labels 
  

plot(ET_hist) # plot the histogram data of bias
```

The time bias /effect not strongly present in distribution. Still have some effect.

##  Combining multiple predictors effect to the simple model

### 2.3.6  Combine Movie and User Effects Model

Due to left and right screw variation in both user and movie bias specific effect, following combination may implies that an additional improvement to our model. $$ 
Y_{u,i} = \mu + e_i + e_u + \varepsilon_{u,i}
$$ Where, $e_u$ is a user-specific effect and $e_i$ is a movie-specific effect $\mu$ and $\varepsilon_{u,i}$ is defined as in (1)

### 2.3.7 Combine Movie + User + Genres Effects Model

The visualize data shows some evidence of a genre effect. If we define $g_{u,i}$ as the genre for user's $u$ rating of movie $i$, the new model will be like following,

$Y_{u,i} = \mu + e_i + e_u + \sum_{k=1}^K x_{u,i} \beta_k + \varepsilon_{u,i}$

Where \$ g_u,i\$ is defined as the genre for user’s u and $x^k_{u,i} = 1$ if $g_{u,i}$ is genre $k$. $\mu$ is defined as in (1) We try to present the model here for explanation, and we will select predictors correlation matrix in following section for model improvement.

## 2.4 Correlation between dependent and independent variables

Before applying machine learning algorithms, it’s important to identify and exclude predictors that offer little value for the task. This preprocessing step focuses on selecting the variables most influential to the target outcome. In this project, a correlation matrix was used to pinpoint the most relevant predictors for model development.

Correlation analysis is useful for understanding the relationships between variables, especially in identifying how strongly they relate to the target variable. In multiple linear regression, high correlations between independent variables can lead to multicollinearity. When two predictors have a correlation coefficient (r) greater than approximately 0.6, it's generally recommended to include only one of them in the model to maintain stability and interpretability.

In addition, we prioritized selecting independent variables that show strong correlations with the dependent variable—movie ratings in this case. The correlation matrix revealed that the user-movie bias/effect model had a strong association with overall ratings. To validate this, we calculated the correlation values between each predictor and the rating, confirming that the user-movie bias/effect model should be the primary focus for further refinement during model optimization.





```{r corelation }
##### Add user bias column to edx and assign new data set for additional coulumns

edx_bias <- edx  %>% left_join(movie_avgs, by='movieId')%>% #left join movie bias data to edx and assign new data set 
  left_join(user_avgs, by='userId')%>% #left join user bias data to edx and assign new data set 
    left_join(gene_avgs,'genres')%>% #left join gene bias data to edx and assign new data set 
  left_join(time_avgs,'date') #left join time bias data to edx and assign new data set 

##### select bias/effect columns for more exploration

edxbias<-edx_bias  %>% select(userId,rating,e_i,e_u,e_g,e_t) # assign new data set and select bias values for new analysis

head(edxbias) # print top six row of data set to check no required value missing

###### Add additional column for correlation analysis

edxbiasall<- edxbias%>% mutate(moviebias=mue+e_i) %>% #assign new data set and put new moviebias column
  mutate(userbias=mue+e_u) %>% #assign new data set and put new userbias column
  mutate(genresbias=mue+e_g) %>% #assign new data set and put new genres bias colum
  mutate(timebias=mue+e_t) %>% #assign new data set and put new timebias column
  mutate (use_mov_bs=mue+e_u+e_i) %>% #assign new data set and put new user moviebias column
  mutate (use_mov_ge_bs= mue+e_u+e_i+e_g) #assign new data set and put new user movie genres bias column
### Correlation check for dependent variable and combine independent variables

corr <- edxbiasall %>% select(rating, moviebias,userbias,genresbias,timebias,use_mov_bs,use_mov_ge_bs)
index <- sample(1:nrow(corr), 1000000)
corr <- corr[index, ]

corrplot(cor(corr), method = "number", type="upper")

###Check correlation value between dependent and independent variables and themself

y<- (edxbiasall$rating)
x<- (edxbiasall$use_mov_bs)
x1<- (edxbiasall$use_mov_ge_bs)

cor(y,x)
cor(y,x1)
cor(x,x1)
```

## 2.5 Evaluation Method of the models

In this project, RMSE (Root Mean Square Error) was used to evaluate and determine the best model. RMSE is interpreted similarly to a standard deviation and represents the typical error made when predicting a movie rating. A lower RMSE value indicates a better-performing machine learning model. The RMSE is calculated as follows:

$$ 
\mbox{RMSE} = \sqrt{\frac{1}{N} \sum_{u,i}^{} \left( \hat{y}_{u,i} - y_{u,i} \right)^2 }
$$

Where, $y_{u,i}$ is the rating for movie $i$ and user $u$ $\hat{y}_{u,i}$ is our prediction $N$ is the number of user/movie combinations sum is over all above combinations

A function that computes the RMSE for vectors of ratings and their corresponding predictors can be written as following code.

```{r root mean sq}

######RMSE (residual mean square error) 

RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```



## 2.6 Regularization

A key challenge in training machine learning models is preventing overfitting—when a model becomes overly complex and starts capturing noise in the training data. This leads to poor generalization and reduced accuracy on unseen data. Overfitting occurs when the model fits specific patterns that don’t hold beyond the training set. Cross-validation is a powerful technique to address this issue. It estimates a model’s performance on unseen data and guides the selection of effective predictors. As a fundamental concept in machine learning, cross-validation ensures robust performance across different data subsets.


Regularization is another effective technique for controlling overfitting. Its core principle is to constrain the variability of effect sizes by adding a penalty term to the standard least squares objective. This discourages overly complex models, reduces prediction error variance, and is especially useful when dealing with highly correlated predictors.


To build the best-performing model, it's crucial to identify the optimal value for the regularization parameter, commonly known as lambda. Lambda controls the strength of regularization, with higher values applying greater penalty to model complexity. Using a non-negative lambda leads to parameter shrinkage, which helps reduce overfitting and improve generalization.

For model regularization, the edX dataset was split into 90% training and 10% validation sets. This partitioning was performed using the caret package in R, which streamlines model training and evaluation through efficient data splitting and cross-validation.

```{r partition }

#### Made data partation for test and train  from edx set for regularization

set.seed(1, sample.kind ="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
train <- edx[-test_index,]
temp <- edx[test_index,]

### Make sure userId and movieId in test set are also in train set
test <- temp %>% 
  semi_join(train, by = "movieId") %>%
  semi_join(train, by = "userId")

### Add rows removed from test set back into train set
removed <- anti_join(temp, test)
train <- rbind(train, removed)
rm(temp,removed,test_index)
```

To identify the optimal lambda value, we evaluated the updated training and validation datasets provided by edX. As illustrated in the figure below, a lambda value of 5 resulted in the lowest RMSE for the regularized model. Consequently, this value was selected for the final model evaluation.

```{r regularization }
#### Regularization of movie + user effect model ##

### Finding optimum parameter in edx 

lmdas <- seq(0, 10, 0.25)

#### below code will take several minutes to run

rmses <- sapply(lmdas, function(l){
  muu <- mean(train$rating)
  b_i <- train %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - muu)/(n()+l))
  b_u <- train %>%
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - b_i - muu)/(n()+l))
  predicted_ratings <- test %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = muu + b_i + b_u) %>%
    .$pred
  
  return(RMSE(predicted_ratings,test$rating))
})

### plot lambda and RMSEs to select optimal Theda


L<-qplot(lmdas, rmses,geom=c("point", "line"))  # assign qplot for lambda and rmses
L+labs(title = "Rmses vs lambdas ") # add the main title for plot

### find optimal lambda (optimun parameter)

lmda <- lmdas[which.min(rmses)] # assign theta value which give minimun rmse value
lmda # print minumun lambda value for rsme

```

# 3. Results and Discussions

After developing the models, a final validation test was performed using the validation dataset. Prior analysis showed that user and movie-related biases had the greatest impact among all predictors. The correlation matrix also identified the most influential variables for model selection during preprocessing. Based on these findings, we hypothesized that combining user and movie effects would produce the lowest RMSE. The RMSE results for all tested models are summarized in the table below.


```{r  all results}
#####TEST all MODELS####
####Put new column of date in validation set
  
  validation <- validation %>% mutate(date = round_date(as_datetime(timestamp), unit = "week"))

### predict all unknown ratings mu with validation rating

mu<-mean(edx$rating)

naive_rmse <- RMSE(validation$rating, mu) # assign rmse value 


### create a table to store results of prediction approaches

rmse_results <- tibble(Method = "The Average", RMSE = naive_rmse) # put result to rmse table and put the name of result

####### Test and save RMSE results with time effect


predicted_ratings <- validation %>% # assign predicted rating  and...
  left_join(time_avgs, by="date") %>% # left join time avgs by date to validation set....
  mutate(pred = mu + e_t) %>% # put new column of predition 
  .$pred # take predit value

mdl_2_rmse <- RMSE(predicted_ratings, validation$rating) # assign rmse value 
rmse_results <- bind_rows(rmse_results,                    # put  result to the rmse result table
                          tibble(Method="Time Effect Model", # put the name of rmse results
                                 RMSE = mdl_2_rmse))

########
##Test and save RMSE results with gene effect

predicted_ratings <- validation %>%  # assign predicted rating  and...
  left_join(gene_avgs, by="genres") %>%   # left join genre avgs by date to validation set....
  mutate(pred = mu + e_g) %>%  # put new column of predition 
  .$pred  # take predit value
mdl_3_rmse <- RMSE(predicted_ratings, validation$rating) # assign rmse value 
rmse_results <- bind_rows(rmse_results,                     # put  result to the rmse result table
                          tibble(Method="Genres Effect Model",  # put the name of rmse result
                                 RMSE = mdl_3_rmse))    

####### Test and save RMSE results with user effect

predicted_ratings <- validation %>% #assign predicted rating  and..
  left_join(user_avgs, by="userId") %>% # left join user avgs by date to validation set....
  mutate(pred = mu + e_u) %>%  # put new column of predition 
  .$pred # take predit value
mdl_4_rmse <- RMSE(predicted_ratings, validation$rating) # assign rmse value 
rmse_results <- bind_rows(rmse_results, # put  result to the rmse result table
                          tibble(Method="User Effect Model",
                                 RMSE = mdl_4_rmse)) # put the name of rmse result




### Test and save RMSE results with movie effect

predicted_ratings <- validation %>% # assign predicted rating  and..
  left_join(movie_avgs, by='movieId') %>% # left join movie avgs by date to validation set....
  mutate(pred = mu + e_i ) %>% # put new column of predition 
  .$pred # take predit value
mdl_5_rmse <- RMSE(predicted_ratings, validation$rating) # assign rmse value 
rmse_results <- bind_rows(rmse_results,    # put  result to the rmse result table
                          tibble(Method="Movie Effect Model",
                                 RMSE = mdl_5_rmse)) # put the name of rmse result




### test and save new RMSE results with user&movie effect


predicted_ratings <- validation %>% #assign predicted rating  and..
  left_join(movie_avgs, by='movieId') %>% # left join movie avgs by date to validation set....
  left_join(user_avgs, by='userId') %>% # left join user avgs by date to validation set....
  mutate(pred = mu + e_i + e_u) %>% # put new column of predition 
  .$pred # take predit value
mdl_6_rmse <- RMSE(predicted_ratings, validation$rating) # assign rmse value 
rmse_results <- bind_rows(rmse_results,   # put  result to the rmse result table
                          tibble(Method = "Combine Movie and User Effects Model",
                                 RMSE = mdl_6_rmse)) # put the name of rmse result


### test and save new RMSE results with regularize user&movie effect with tuning parameter (lambda)

lmda <- 5
mu <- mean(edx$rating)
movie_reg_avgs <- edx %>%
  group_by(movieId) %>%
  summarize(e_i = sum(rating - mu)/(n()+lmda)) 

user_reg_avgs <- edx %>% 
  left_join(movie_reg_avgs, by="movieId") %>%
  group_by(userId) %>%
  summarize(e_u = sum(rating - mu-e_i)/(n()+lmda))

predicted_ratings <- validation %>% #assign predicted rating  and..
  left_join(movie_reg_avgs, by='movieId') %>% # left join movie avgs by date to validation set....
  left_join(user_reg_avgs, by='userId') %>% # left join user avgs by date to validation set....
  mutate(pred = mu + e_i + e_u) %>% # put new column of predition 
  .$pred # take predit value

mdl_7_rmse  <- RMSE(predicted_ratings, validation$rating)

rmse_results <- bind_rows(rmse_results, # assign rmse value and put  result to the rmse result table
                          tibble(Method = "Regularized combine Movie and User Effects Model",
                                 RMSE = mdl_7_rmse)) # put the name of rmse result

### table showing all model final test results

rmse_results %>% knitr::kable()



```

The results of the various models built to predict movie ratings, measured by Root Mean Squared Error (RMSE), demonstrate a clear trend of improvement as more complex effects are incorporated into the models. Here's a breakdown of the performance for each model:

1.Average Model (RMSE = 1.0612018): This is the baseline model that predicts ratings by simply using the average rating across all movies. Its RMSE value is the highest, indicating that predicting the mean rating for all users does not provide a good fit for the data.

2.Time Effect Model (RMSE = 1.0575018): By incorporating the time effect, this model slightly improves upon the average model. The time effect takes into account the trend in ratings over time, which may reflect shifting user preferences, but the improvement is marginal.

3.Genres Effect Model (RMSE = 1.0184056): This model introduces the genre of the movie as a predictor, showing a noticeable improvement in RMSE. The fact that genre influences ratings suggests that users have preferences for specific genres, and this factor helps refine predictions.

4.User Effect Model (RMSE = 0.9783360): Incorporating user-specific effects, this model improves further. This indicates that individual user preferences play a significant role in predicting ratings, as users tend to rate movies based on their unique tastes.

5.Movie Effect Model (RMSE = 0.9439087): Adding movie-specific effects reduces the RMSE even more, indicating that the popularity or inherent characteristics of the movie also impact how users rate them. This model takes into account the biases related to individual movies.

6.Combine Movie and User Effects Model (RMSE = 0.8850398): When combining both movie and user effects, the model performs significantly better than its predecessors. This indicates that both the user’s preferences and the characteristics of the movie together provide more accurate predictions.

7.Regularized Combine Movie and User Effects Model (RMSE = 0.8648177): The final model, which includes both regularization and the combined movie and user effects, yields the best performance with the lowest RMSE. Regularization helps reduce overfitting by penalizing overly complex models, leading to better generalization on unseen data.

In summary, as the model incorporates more complex features—such as user preferences, movie attributes, and their interactions—the RMSE consistently decreases, indicating improved predictive accuracy. The final regularized model, which combines both movie and user effects, achieves the lowest RMSE, making it the most accurate among all models evaluated.



# 4. Conclusion and recommendation
This project was successfully implemented and surpassed the initial goal of predicting movie ratings using the 10 million MovieLens dataset. After splitting the data into the edx and validation sets for training and testing purposes, we began with thorough data exploration. The dataset was then cleaned, wrangled, and visualized to extract meaningful insights. Understanding data correlations played a crucial role in enhancing the predictive performance of the model. To further improve accuracy, regularization was applied with a lambda value of 5, resulting in a more robust prediction model. As a result, the final model was able to achieve the target RMSE, fulfilling the objectives of the Capstone project.

###  Potential Impact

Today, recommendation engines and machine learning are widely deployed across platforms such as YouTube, Facebook, LinkedIn, Netflix, Alibaba, as well as various web, mobile, and mini applications. These technologies play a significant role in driving digital transformation and accelerating the growth of e-commerce.



###   Limitation

From this Capstone project, we have gained valuable insights along with a better understanding of its limitations. One key takeaway is the importance of internet speed and having a fast, high-memory laptop to ensure smooth execution and efficient data analysis. Additionally, we recognized that the project has limitations in terms of the variables considered. For a more realistic and personalized recommendation system—whether for movies or businesses—factors such as user behavior, movie language, user age group (child or adult), celebrity popularity, current trends, geographical location, and socio-economic status (to estimate spending habits) are critical. Moreover, the content and context of the movies themselves play an essential role in tailoring recommendations to individual users. Incorporating these aspects would lead to deeper insights and a more customized user experience.

###  Future work

Finally, the capstone project was successfully deployed to achieved the desired target, It is likely that more efficient Neural Network and Machining Learning  models are recommended for future iterations of models involving K-Nearest-Neighbors and Collaborative Filtering with content base and context to improve the overall user experience for streamers everywhere.



# 5.References

1.Irizarry, R., 2019. Introduction To Data Science. [online] Rafalab.github.io. Available at: rafalab.github.io/dsbook.

2.“MovieLens 10M Dataset.” GroupLens, 2019, Available at: grouplens.org/datasets/ movielens/10m/.

3. Jannach, S., and Adomavicius, G., "Recommender Systems: Challenges and Research Opportunities,"
Springer, 2016.

4. Karypis, G., and Rajan, Y., "Collaborative Filtering Recommender Systems," Springer, 2011.

5. Shani, B. S., and Gunawardana, A., "Evaluating recommendation systems," in Recommender Systems
Handbook, Springer, pp. 257-297, 2011.

6. Ricci, M., Rokach, L., and Shapira, B., "Introduction to Recommender Systems Handbook," Springer,
2015.

7. Koren, Y., Bell, R., and Volinsky, C., "Matrix Factorization Techniques for Recommender Systems," IEEE
Computer, vol. 42, no. 8, pp. 30-37, 2009.





